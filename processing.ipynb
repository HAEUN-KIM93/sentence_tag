{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e016f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cd09dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 104743\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 5463\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 5463\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"qnli\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3e1af0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, Joseph Haas was arrested for allegedly sending an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"\n"
     ]
    }
   ],
   "source": [
    "sen='For example, Joseph Haas was arrested for allegedly sending an email to the Lebanon, New Hampshire city councilors stating, \"Wise up or die.\"'\n",
    "if sen in dataset['validation']['sentence'] or dataset['test']['sentence']:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737fad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880649e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ast, re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "OUTPUT_BLOCK = re.compile(r\"(?:Sample\\s+)?Output\\s*:\\s*(\\[[\\s\\S]*?\\])\", re.IGNORECASE)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \n",
    "    s = re.sub(r\"\\s+\", \" \", str(s))\n",
    "    s = re.sub(r\"\\s+([.,;:])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([.,;:])\\s*\", r\"\\1 \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def _loads_list_any(s: str):\n",
    "    \"\"\"JSON -> ast.literal_eval parsing if fail None.\"\"\"\n",
    "    for loader in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            obj = loader(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _is_empty_like(x) -> bool:\n",
    "    \"\"\"NaN/None/'nan'/'null'/'none'/'[]' ->empty.\"\"\"\n",
    "    if x is None:\n",
    "        return True\n",
    "    try:\n",
    "        \n",
    "        from math import isnan\n",
    "        if isinstance(x, float) and isnan(x):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip()\n",
    "    if s == \"\": return True\n",
    "    if s.lower() in {\"nan\", \"null\", \"none\"}: return True\n",
    "    if s == \"[]\": return True\n",
    "    return False\n",
    "def parse_pred(cell):\n",
    "    \"\"\"\n",
    "    pred:\n",
    "      1) 'Output:' if block only use the content in [ ... ]\n",
    "      2) parsing [\"...\"] 리스트로 파싱 시도(LLM 출력 호환)\n",
    "      3) 1 fail ,2 fail: empty→[],or single string\n",
    "    return: (list[str], ok:bool)  # ok=True, parsing success\n",
    "    \"\"\"\n",
    "    if _is_empty_like(cell):\n",
    "        return []\n",
    "    s = str(cell).strip()\n",
    "\n",
    "    m = OUTPUT_BLOCK.search(s)\n",
    "    if m:\n",
    "        block = m.group(1).strip()\n",
    "        obj = _loads_list_any(block)\n",
    "        if obj is None:\n",
    "            return [], False\n",
    "        return [normalize_text(str(it)) for it in obj]\n",
    "\n",
    "    obj = _loads_list_any(s)\n",
    "    if obj is not None:\n",
    "        return [normalize_text(str(it)) for it in obj]\n",
    "\n",
    "    return [normalize_text(s)]\n",
    "def parse_gold(cell):\n",
    "    \"\"\"\n",
    "    gold:\n",
    "      - if list of json -> parsing \n",
    "      - or empty -> []], or single string \n",
    "    return: (list[str], ok:bool)\n",
    "    \"\"\"\n",
    "    if _is_empty_like(cell):\n",
    "        return []\n",
    "    if isinstance(cell, list):\n",
    "        return [normalize_text(str(x)) for x in cell]\n",
    "    s = str(cell).strip()\n",
    "    obj = _loads_list_any(s)\n",
    "    if obj is not None:\n",
    "        return [normalize_text(str(it)) for it in obj]\n",
    "    return [normalize_text(s)]\n",
    "\n",
    "def processing_from_file(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    if \"idx\" not in df.columns:\n",
    "        df = df.reset_index().rename(columns={\"index\": \"idx\"})\n",
    "    def parse_row(row):\n",
    "        gold = parse_gold(row.get(\"gold\", \"\") )\n",
    "        pred = parse_pred(row.get(\"pred\", \"\") )\n",
    "        tag  = row.get('tag', None)\n",
    "        return pd.Series({\n",
    "            \"idx\": row[\"idx\"],\n",
    "            \"gold\": gold,\n",
    "            \"pred\": pred,\n",
    "            \"tag\": tag\n",
    "        })\n",
    "    df_parse = df.apply(parse_row, axis=1)\n",
    "    return df_parse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ede1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1ab92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-Llama\n",
    "1.llama_no_training\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\llama_no_training\n",
    "2.llama_sft20\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20_sft\\strong_prompt_soft_model\n",
    "3.llama_curriculum_dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\curriculum_dpo\\strong_prompt_soft_model\n",
    "4.llama_dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_only\\strong_prompt_strong_model\n",
    "5.llama_sft20+curriculum+dpo(soft)\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20\\strong_prompt_soft_model\n",
    "6.llama sft20+curriculum+dpo(strong)\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample_compare\\strong_prompt_strong_model\n",
    "-Phi\n",
    "1.phi_no_training\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_no_training\n",
    "2.phi_sft20\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_sample_sft20\\strong_prompt_strong_model\n",
    "3-1.phi_curriculum_dpo(strong)\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_no_sft\\strong_prompt_strong_model\n",
    "3-2.phi_curriculum_dpo(soft model)\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_compare\\strong_prompt_soft_model\n",
    "4. Dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_dpo\\strong_prompt_strong_model\n",
    "5.phi_sft20+curriculum+dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_sample20\\strong_prompt_strong_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "aa45919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path=r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\sft_curriculumnodpo\"\n",
    "pattern=os.path.join(current_path,\"shard*.csv\")\n",
    "file_paths = sorted(glob.glob(pattern))\n",
    "postprocessing_paths=os.path.join(current_path,\"post_processing.csv\")\n",
    "#saved_result=r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_no_sft\\strong_prompt_strong_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6b105faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_0.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_1.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_2.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_3.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_4.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_5.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_6.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_7.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_8.csv',\n",
       " 'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo\\\\shard_9.csv']"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49583de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         idx                                               gold  \\\n",
      "0          0  [As of that day, the new constitution heraldin...   \n",
      "1          1                                                 []   \n",
      "2          2                                                 []   \n",
      "3          3  [As of that day, the new constitution heraldin...   \n",
      "4          4                                                 []   \n",
      "...      ...                                                ...   \n",
      "30665  30665  [\" Official \" reconstructions have also been r...   \n",
      "30666  30666                                                 []   \n",
      "30667  30667                                                 []   \n",
      "30668  30668  [\" Official \" reconstructions have also been r...   \n",
      "30669  30669                                                 []   \n",
      "\n",
      "                                                    pred tag  \n",
      "0      [As of that day, the new constitution heraldin...   C  \n",
      "1      [As of that day, the new constitution heraldin...  DC  \n",
      "2                                                     []  FC  \n",
      "3      [As of that day, the new constitution heraldin...   T  \n",
      "4                                                     []  CT  \n",
      "...                                                  ...  ..  \n",
      "30665  [\" Official \" reconstructions have also been r...   C  \n",
      "30666                                                 []  DC  \n",
      "30667                                                 []  FC  \n",
      "30668  [\" Official \" reconstructions have also been r...   T  \n",
      "30669                                                 []  CT  \n",
      "\n",
      "[30670 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\curriculum_strong_4targets_left\"\n",
    "\n",
    "combined =pd.concat([pd.read_csv(path) for path in file_paths],ignore_index=True)\n",
    "  \n",
    "\n",
    "tag_map ={ 0 :'C', 1 : \"DC\" , 2 : 'FC', 3 : 'T',4 : 'CT'}\n",
    "combined['tag'] = combined.index%5\n",
    "combined['tag'] = combined['tag'].map(tag_map)\n",
    "combined_processing=processing_from_file(combined)\n",
    "\n",
    "print(combined_processing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c0212282",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_processing.to_csv(postprocessing_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "312a2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths={\n",
    "\"example\":r\"C:\\Users\\haeun\\Downloads\\hw\\langchain-kr\\Lingustic\\model_finetune\\two_step\\langgraph_project\\llama_example.partial.csv\",\n",
    "\"vanila\":r\"C:\\Users\\haeun\\Downloads\\hw\\langchain-kr\\Lingustic\\model_finetune\\two_step\\langgraph_project\\llama_vanila.partial.csv\",\n",
    "\"parse\":r\"C:\\Users\\haeun\\Downloads\\hw\\langchain-kr\\Lingustic\\model_finetune\\two_step\\langgraph_project\\llama_parse.partial.csv\",\n",
    "\"annotated\":r\"C:\\Users\\haeun\\Downloads\\hw\\langchain-kr\\Lingustic\\model_finetune\\two_step\\langgraph_project\\llama_annotated.partial.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fc8a0",
   "metadata": {},
   "source": [
    "LLama\n",
    "1.curriculum_Dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\curriculum_dpo\\strong_prompt_soft_model\n",
    "2.dpo_only\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_only\\strong_prompt_strong_model\\*.csv\n",
    "3.sample_20\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20\\strong_prompt_soft_model\n",
    "4-1 dpo-sample-compare(strong prompt soft model)\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample_compare\\strong_prompt_soft_model(now)\n",
    "4-2 dpo_sample_compare(strong prompt strong model)\n",
    "5. dpo_sample 20 sft\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20_sft\\strong_prompt_soft_model\n",
    "phi\n",
    "6.phi_no_training\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_no_training\n",
    "1.sample_20\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_sample_sft20\\strong_prompt_strong_model\n",
    "2-1. curriculum + dpoC:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_no_sft\\strong_prompt_strong_model\n",
    "2-2 curriculum+dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_compare\\strong_prompt_soft_model\n",
    "3.sample 20 sft+ curriculum+ dpo\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_sample20\\strong_prompt_strong_model\n",
    "4. dpo_Only\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_dpo\\strong_prompt_strong_model\n",
    "5.llama no training\n",
    "\n",
    "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\llama_no_training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f30b5",
   "metadata": {},
   "source": [
    "# token based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_results=[r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\llama_no_training\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20_sft\\strong_prompt_soft_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\curriculum_dpo\\strong_prompt_soft_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_only\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample20\\strong_prompt_soft_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\dpo_sample_compare\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_no_training\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_sample_sft20\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_no_sft\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_compare\\strong_prompt_soft_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_dpo\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\phi_curriculum_dpo_sample20\\strong_prompt_strong_model\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\after_final\\eval_result_llama_finetune\",\n",
    "r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\after_final\\eval_result_phi_finetune\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b0ac3891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\haeun\\\\Downloads\\\\hw\\\\computer_share\\\\dpo_models\\\\output\\\\phi\\\\sft_curriculumnodpo'"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved=r\"C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\sft_curriculumnodpo\\post_processing.csv\"\n",
    "saved_file=\"\\\\\".join(saved.split(\"\\\\\")[:-1])\n",
    "saved_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haeun\\Downloads\\hw\\computer_share\\dpo_models\\output\\phi\\sft_curriculumnodpo\\post_processing.csv\n",
      "[TAG span macro]\n",
      "      precision    recall        f1  count\n",
      "tag                                      \n",
      "C     0.345578  0.267790  0.291484   6134\n",
      "CT    0.843128  0.803195  0.815464   6134\n",
      "DC    0.829162  0.824474  0.824194   6134\n",
      "FC    0.989077  0.989077  0.989077   6134\n",
      "T     0.810483  0.810564  0.810510   6134\n",
      "[TAG token macro]\n",
      "      precision    recall        f1   jaccard  count\n",
      "tag                                                \n",
      "C     0.809762  0.664270  0.689352  0.599629   6134\n",
      "CT    0.910472  0.854701  0.871824  0.852960   6134\n",
      "DC    0.883831  0.885906  0.877438  0.862949   6134\n",
      "FC    0.989077  0.989077  0.989077  0.989240   6134\n",
      "T     0.891902  0.888252  0.889979  0.884593   6134\n"
     ]
    }
   ],
   "source": [
    "#llama \n",
    "import json, ast, re, argparse\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple\n",
    "OUTPUT_BLOCK = re.compile(r\"(?:Sample\\s+)?Output\\s*:\\s*(\\[[\\s\\S]*?\\])\", re.IGNORECASE)\n",
    "WORD = re.compile(r\"\\w+('\\w+)?\", re.UNICODE)\n",
    "def normalize(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s))\n",
    "    s = re.sub(r\"\\s+([.,;:])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"([.,;:])\\s*\", r\"\\1 \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def is_empty_like(x):\n",
    "    if x is None: return True\n",
    "    if isinstance(x, float) and math.isnan(x): return True\n",
    "    s = str(x).strip()\n",
    "    return s == \"\" or s.lower() in {\"nan\",\"null\",\"none\"}\n",
    "\n",
    "def loads_list_any(s: str):\n",
    "    for loader in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            obj = loader(s)\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "def to_list(cell, is_pred=False):\n",
    "   \n",
    "    if is_empty_like(cell): \n",
    "        return []\n",
    "    if isinstance(cell, list):\n",
    "        return [normalize(str(v)) for v in cell]\n",
    "\n",
    "    s = str(cell).strip()\n",
    "\n",
    "    \n",
    "    if is_pred:\n",
    "        m = OUTPUT_BLOCK.search(s)\n",
    "        if m:\n",
    "            s = m.group(1).strip()  \n",
    "\n",
    "    # (JSON/리터럴) parsing\n",
    "    obj = loads_list_any(s)\n",
    "    if isinstance(obj, list):\n",
    "        return [normalize(str(v)) for v in obj]\n",
    "\n",
    "    \n",
    "    return [normalize(s)]\n",
    "\n",
    "\n",
    "def tokenize_spans(spans, lower=True):\n",
    " \n",
    "    toks = []\n",
    "    for s in spans:\n",
    "        s = s if isinstance(s, str) else str(s)\n",
    "        if lower:\n",
    "            s = s.lower()\n",
    "        toks.extend(m.group(0) for m in WORD.finditer(s))\n",
    "    return toks\n",
    "\n",
    "def prf_from_counts(tp, fp, fn):\n",
    "    prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "def row_metrics_span(gold_spans, pred_spans,iou_tau=None, ):\n",
    "    \n",
    "    if not gold_spans and not pred_spans:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    gc, pc = Counter(gold_spans), Counter(pred_spans)\n",
    "    tp = sum((gc & pc).values())\n",
    "    fp = sum(pc.values()) - tp\n",
    "    fn = sum(gc.values()) - tp\n",
    "    return prf_from_counts(tp, fp, fn)\n",
    "def row_metrics_token(gold_spans, pred_spans,):\n",
    "    if not gold_spans and not pred_spans:\n",
    "        return 1.0, 1.0, 1.0,1.0\n",
    "    gt, pt = Counter(tokenize_spans(gold_spans)), Counter(tokenize_spans(pred_spans))\n",
    "    tp = sum((gt & pt).values())\n",
    "    fp = sum(pt.values()) - tp\n",
    "    fn = sum(gt.values()) - tp\n",
    "    precision, recall, f1 = prf_from_counts(tp, fp, fn)\n",
    "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) else 1.0\n",
    "    return precision, recall, f1, jaccard\n",
    "def load_test_with_sentence():\n",
    "    ds = load_dataset(\"haeunkim/curriculum_learning\", split=\"test\")\n",
    "    df_test = ds.to_pandas()\n",
    "    # sentence column extract: priority in order: 'input' → 'sentence' → 'text' 등\n",
    "    for col in [\"input\"]:\n",
    "        if col in df_test.columns:\n",
    "            sent_col = col\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"cannot find columns. columns={list(df_test.columns)[:10]}\")\n",
    "    out = df_test[[sent_col]].copy()\n",
    "    out = out.rename(columns={sent_col: \"sentence\"})\n",
    "    out[\"idx\"] = range(len(out))\n",
    "    return out\n",
    "def all_occurs(text: str, span: str, *, overlapping=True, ignore_case=False) -> List[Tuple[int,int]]:\n",
    "    \n",
    "    if not span:\n",
    "        return []\n",
    "    flags = re.DOTALL | (re.IGNORECASE if ignore_case else 0)\n",
    "    lit = re.escape(span)  \n",
    "    if overlapping:\n",
    "        rx = re.compile(rf\"(?={lit})\", flags)\n",
    "        return [(m.start(), m.start()+len(span)) for m in rx.finditer(text)]\n",
    "    else:\n",
    "        rx = re.compile(lit, flags)\n",
    "        return [(m.start(), m.end()) for m in rx.finditer(text)]\n",
    "def tagwise_both(df, gold_col=\"gold\" , pred_col=\"pred\", tag_col=\"tag\"):\n",
    "    \"\"\"\n",
    "    return:\n",
    "      rows_df: rowsise p/r/f1 (span & token)\n",
    "      tag_span: macro(average) PRF + n\n",
    "      tag_token: macro(average) PRF + n\n",
    "    \"\"\"\n",
    "    test_df = load_test_with_sentence()\n",
    "    df = df.merge(test_df, on=\"idx\", how=\"left\")\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        gold = to_list(r.get(gold_col, \"\"), is_pred=False)\n",
    "        pred = to_list(r.get(pred_col, \"\"), is_pred=True)\n",
    "        p_s, r_s, f1_s = row_metrics_span(gold, pred)\n",
    "        p_t, r_t, f1_t ,J= row_metrics_token(gold, pred)\n",
    "        rows.append({\n",
    "            \"tag\": r.get(tag_col, None),\n",
    "            \"precision_span\": p_s, \"recall_span\": r_s, \"f1_span\": f1_s,\n",
    "            \"precision_tok\":  p_t, \"recall_tok\":  r_t, \"f1_tok\":  f1_t,\n",
    "            \"jaccard_tok\": J\n",
    "        })\n",
    "    rows_df = pd.DataFrame(rows)\n",
    "\n",
    "    # macro averate per tag\n",
    "    tag_span = rows_df.groupby(\"tag\")[[\"precision_span\",\"recall_span\",\"f1_span\"]].mean() \\\n",
    "                      .rename(columns={\"precision_span\":\"precision\",\"recall_span\":\"recall\",\"f1_span\":\"f1\"})\n",
    "    tag_token = rows_df.groupby(\"tag\")[[\"precision_tok\",\"recall_tok\",\"f1_tok\",\"jaccard_tok\"]].mean() \\\n",
    "                       .rename(columns={\"precision_tok\":\"precision\",\"recall_tok\":\"recall\",\"f1_tok\":\"f1\",\"jaccard_tok\":\"jaccard\"})\n",
    "    \n",
    "    counts = rows_df.groupby(\"tag\").size().rename(\"count\")\n",
    "    tag_span = tag_span.join(counts)\n",
    "    tag_token = tag_token.join(counts)\n",
    "    \n",
    "    return rows_df, tag_span, tag_token\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv(saved)\n",
    "rows_df, tag_span, tag_token = tagwise_both(df)\n",
    "print(saved)\n",
    "print(\"[TAG span macro]\\n\", tag_span)\n",
    "print(\"[TAG token macro]\\n\", tag_token)\n",
    "rows_df.to_csv(os.path.join(saved_file,\"rows_per_example.csv\"), index=False)\n",
    "tag_span.to_csv(os.path.join(saved_file,\"tag_span_macro.csv\"))\n",
    "tag_token.to_csv(os.path.join(saved_file,\"tag_token_macro.csv\"))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f83bf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#negative sample\n",
    "\n",
    "def left_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[1:]) if len(toks) >= 2 else \"\"\n",
    "\n",
    "def right_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[:-1]) if len(toks) >= 2 else \"\"\n",
    "\n",
    "def both_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[1:-1]) if len(toks) >= 3 else \"\"\n",
    "def get_negative_sample(sentence,chosen):\n",
    "   \n",
    "    choice=random.randint(0,4)\n",
    "    len_sentence=len(sentence)\n",
    "    if choice == 0:\n",
    "        \n",
    "        repeat =random.randint(0,5)\n",
    "        rejected=[sentence] * repeat\n",
    "        \n",
    "        out =  json.dumps(rejected, ensure_ascii=False)\n",
    "    elif choice == 1:\n",
    "        \n",
    "        start =random.randint(0,len_sentence)\n",
    "        end =random.randint(0,len_sentence)\n",
    "        out = json.dumps([{\"start\": start, \"end\": end}], ensure_ascii=False)\n",
    "    elif choice == 2:\n",
    "        repeat =random.randint(0,5)\n",
    "        rejected=[]\n",
    "        for i in range(repeat):\n",
    "            start =random.randint(0,len_sentence)\n",
    "            end =random.randint(start,len_sentence)  \n",
    "            rejected.append(sentence[start:end])  \n",
    "        out = json.dumps(rejected, ensure_ascii=False) if rejected else \"[]\"\n",
    "    elif choice == 3:\n",
    "        rejected=[]\n",
    "        kind = random.choice([\"broken_left\", \"broken_right\"])\n",
    "        start =random.randint(0,len_sentence+1)\n",
    "        end =random.randint(start,len_sentence+1)\n",
    "        frag = sentence[start:end]  \n",
    "        if kind =='broken_right':\n",
    "           out= f'[\"{frag}'\n",
    "        else:\n",
    "            out= f'{frag}\"]'\n",
    "    elif choice==4:\n",
    "        rejected=[]\n",
    "        for gold in chosen:\n",
    "            rn=random.randint(0,3)\n",
    "            if rn==0:\n",
    "                corrupted = left_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "            elif rn==1:\n",
    "                corrupted =right_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "            else:\n",
    "                corrupted = both_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "        out= json.dumps(rejected, ensure_ascii=False) if rejected else \"[]\"\n",
    "    if out.strip() == chosen.strip():\n",
    "        out =get_negative_sample(sentence,chosen)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets,load_dataset\n",
    "data_low=load_dataset(\"haeunkim/curriculum_learning\",split='train_low')\n",
    "data_medium=load_dataset(\"haeunkim/curriculum_learning\",split='train_medium')\n",
    "data_high=load_dataset(\"haeunkim/curriculum_learning\",split='train_high')\n",
    "data_low_after=concatenate_datasets([data_low,data_medium,data_high])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "2f427f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda44d4966864dbcaad2ce4585340559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/99450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac06b8af834bb498309432abcb87d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/99195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9218424dce14f79939b7bd4e23c3c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/96110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "def add_negative_row(example):\n",
    "    return {\n",
    "        \"rejected\": get_negative_sample(\n",
    "            sentence=example[\"input\"],\n",
    "            chosen=example[\"output\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "processed_data = data_low.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "\n",
    "processed_medium = data_medium.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "\n",
    "processed_test = data_high.map(add_negative_row, desc=\"Generating negative samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49dde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a75da4024d44c6a2be259ba3985271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed3b63af5134672a7d55143acd7af75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220d57d5f1e24db58b56f96db8356ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12358346e594762b40da35748e52733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a5b85334e48308471c6cb0a96fbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b774ad7b604a73a7ff93341cc8ee8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/97 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/haeunkim/curriculum_learning_dpo_created_negative/commit/449176960cf40a59569b88b1b5cf87c38b43a9c1', commit_message='Upload dataset', commit_description='', oid='449176960cf40a59569b88b1b5cf87c38b43a9c1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/haeunkim/curriculum_learning_dpo_created_negative', endpoint='https://huggingface.co', repo_type='dataset', repo_id='haeunkim/curriculum_learning_dpo_created_negative'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Value\n",
    "dset = DatasetDict({\n",
    "    \"train_low\":    processed_data,    \n",
    "    \"train_medium\": processed_medium,  \n",
    "    \"train_high\":   processed_test,    \n",
    "})\n",
    "repo_id = \"haeunkim/curriculum_learning_dpo_created_negative\"\n",
    "dset.push_to_hub(\n",
    "    repo_id,\n",
    "    private=True,              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "30d4065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'apple']\n",
      "['\"you', 'like\",', 'a', 'pear']\n"
     ]
    }
   ],
   "source": [
    "ap=['I like apple',' \"you like\", a pear']\n",
    "for span in ap:\n",
    "    print(span.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
