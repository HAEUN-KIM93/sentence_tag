{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a40803",
   "metadata": {},
   "source": [
    "# DATASET FOR CURRICULUM LEARNING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c650a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\haeun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m benepar\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenepar_en3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbenepar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbenepar_en3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 3) parse tree \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_parse\u001b[39m(sentence):\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\spacy\\language.py:821\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    817\u001b[0m     pipe_component, factory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    818\u001b[0m         factory_name, source, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 821\u001b[0m     pipe_component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfactory_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m pipe_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_meta[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\spacy\\language.py:709\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    706\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {factory_name: config}\n\u001b[0;32m    707\u001b[0m \u001b[38;5;66;03m# We're calling the internal _fill here to avoid constructing the\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# registered functions twice\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m resolved \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m filled \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mfill({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg[factory_name]}, validate\u001b[38;5;241m=\u001b[39mvalidate)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcfg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    711\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled)\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\confection\\__init__.py:760\u001b[0m, in \u001b[0;36mregistry.resolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m     validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    759\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 760\u001b[0m     resolved, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\confection\\__init__.py:809\u001b[0m, in \u001b[0;36mregistry._make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interpolated:\n\u001b[0;32m    808\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config(orig_config)\u001b[38;5;241m.\u001b[39minterpolate()\n\u001b[1;32m--> 809\u001b[0m filled, _, resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolve\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m filled \u001b[38;5;241m=\u001b[39m Config(filled, section_order\u001b[38;5;241m=\u001b[39msection_order)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;66;03m# Check that overrides didn't include invalid properties not in config\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\confection\\__init__.py:881\u001b[0m, in \u001b[0;36mregistry._fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    878\u001b[0m     getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget(reg_name, func_name)\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;66;03m# We don't want to try/except this and raise our own error\u001b[39;00m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# here, because we want the traceback if the function fails.\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     getter_result \u001b[38;5;241m=\u001b[39m \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;66;03m# We're not resolving and calling the function, so replace\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# the getter_result with a Promise class\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     getter_result \u001b[38;5;241m=\u001b[39m Promise(\n\u001b[0;32m    886\u001b[0m         registry\u001b[38;5;241m=\u001b[39mreg_name, name\u001b[38;5;241m=\u001b[39mfunc_name, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    887\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\benepar\\integrations\\spacy_plugin.py:176\u001b[0m, in \u001b[0;36mcreate_benepar_component\u001b[1;34m(nlp, name, model, subbatch_max_tokens, disable_tagger)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_benepar_component\u001b[39m(\n\u001b[0;32m    170\u001b[0m     nlp,\n\u001b[0;32m    171\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     disable_tagger: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m    175\u001b[0m ):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBeneparComponent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubbatch_max_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubbatch_max_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tagger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tagger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\benepar\\integrations\\spacy_plugin.py:116\u001b[0m, in \u001b[0;36mBeneparComponent.__init__\u001b[1;34m(self, name, subbatch_max_tokens, disable_tagger, batch_size)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    102\u001b[0m ):\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a trained parser model.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m        batch_size: deprecated and ignored; use subbatch_max_tokens instead\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser \u001b[38;5;241m=\u001b[39m \u001b[43mload_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parser\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\benepar\\integrations\\downloader.py:34\u001b[0m, in \u001b[0;36mload_trained_model\u001b[1;34m(model_name_or_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m model_path \u001b[38;5;241m=\u001b[39m locate_model(model_name_or_path)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_chart\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChartParser\n\u001b[1;32m---> 34\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mChartParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_trained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\benepar\\parse_chart.py:185\u001b[0m, in \u001b[0;36mChartParser.from_trained\u001b[1;34m(cls, model_path)\u001b[0m\n\u001b[0;32m    182\u001b[0m     hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_root_constituent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    184\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m nkutil\u001b[38;5;241m.\u001b[39mHParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhparams)\n\u001b[1;32m--> 185\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m parser\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\benepar\\parse_chart.py:73\u001b[0m, in \u001b[0;36mChartParser.__init__\u001b[1;34m(self, tag_vocab, label_vocab, char_vocab, hparams, pretrained_model_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretokenizer \u001b[38;5;241m=\u001b[39m retokenization\u001b[38;5;241m.\u001b[39mRetokenizer(\n\u001b[0;32m     71\u001b[0m         pretrained_model_path, retain_start_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     )\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m d_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hparams\u001b[38;5;241m.\u001b[39muse_encoder:\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:438\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[1;34m(cls, config, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    437\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\modeling_utils.py:1503\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[1;34m(cls, config, **kwargs)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1503\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m# restore default dtype if it was modified\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1363\u001b[0m, in \u001b[0;36mT5Model.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m T5Stack(decoder_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared)\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m-> 1363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;66;03m# Model parallel\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\modeling_utils.py:1406\u001b[0m, in \u001b[0;36mPreTrainedModel.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_init\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;124;03m    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;124;03m    modules properly initialized (such as weight initialization).\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1406\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_compatibility_gradient_checkpointing()\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\modeling_utils.py:2321\u001b[0m, in \u001b[0;36mPreTrainedModel.init_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads)\n\u001b[0;32m   2319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_weights:\n\u001b[0;32m   2320\u001b[0m     \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[1;32m-> 2321\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2323\u001b[0m     \u001b[38;5;66;03m# Tie weights should be skipped when not initializing all weights\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;66;03m# since from_pretrained(...) calls tie weights anyways\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:891\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m    890\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m--> 891\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\modeling_utils.py:1819\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1819\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haeun\\anaconda3\\envs\\project\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:821\u001b[0m, in \u001b[0;36mT5PreTrainedModel._init_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    814\u001b[0m     module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    816\u001b[0m     module,\n\u001b[0;32m    817\u001b[0m     (T5Model, T5ForConditionalGeneration, T5EncoderModel, T5ForQuestionAnswering),\n\u001b[0;32m    818\u001b[0m ):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;66;03m# Mesh TensorFlow embeddings initialization\u001b[39;00m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\u001b[39;00m\n\u001b[1;32m--> 821\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[0;32m    823\u001b[0m         module\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, std\u001b[38;5;241m=\u001b[39mfactor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import spacy\n",
    "import benepar\n",
    "from tqdm import tqdm\n",
    "\n",
    "data=load_dataset(\"haeunkim/sentence_tag_multitask_v2\")\n",
    "\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "# 3) parse tree \n",
    "def get_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    return list(doc.sents)[0]._.parse_string\n",
    "\n",
    "# 4) depth\n",
    "def get_tree_depth(parse_str: str) -> int:\n",
    "    depth = 0\n",
    "    max_depth = 0\n",
    "    for ch in parse_str:\n",
    "        if ch == \"(\":\n",
    "            depth += 1\n",
    "            max_depth = max(max_depth, depth)\n",
    "        elif ch == \")\":\n",
    "            depth -= 1\n",
    "    return max_depth\n",
    "\n",
    "# 5) hypothesis parse & depth \n",
    "parses = []\n",
    "depths = []\n",
    "\n",
    "for sent in tqdm(data[\"train\"][\"input\"]):  \n",
    "    parse_str = get_parse(sent)\n",
    "    parses.append(parse_str)\n",
    "    depths.append(get_tree_depth(parse_str))\n",
    "\n",
    "# 6) adding columns to train dataset\n",
    "data[\"train\"] = data[\"train\"].add_column(\"parse\", parses)\n",
    "data[\"train\"] = data[\"train\"].add_column(\"hypo_depth\", depths)\n",
    "parses = []\n",
    "depths = []\n",
    "\n",
    "for sent in tqdm(data[\"test\"][\"input\"]): \n",
    "    parse_str = get_parse(sent)\n",
    "    parses.append(parse_str)\n",
    "    depths.append(get_tree_depth(parse_str))\n",
    "\n",
    "# 6) adding columns to test dataset\n",
    "data[\"test\"] = data[\"test\"].add_column(\"parse\", parses)\n",
    "data[\"test\"] = data[\"test\"].add_column(\"hypo_depth\", depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split,ds in old_data.items():\n",
    "    print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a70c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i correct the wrong instruction \n",
    "from datasets import DatasetDict\n",
    "DC_TEXT = \"Extract all DC clauses(dependent clauses) in the sentence. Return a list of clause spans. If no such clause exists, return [].\"\n",
    "def rewrite_instr(ds):\n",
    "    return ds.map(\n",
    "        lambda ex, idx: {\"instruction\": DC_TEXT if (idx % 5 == 1) else ex[\"instruction\"]},\n",
    "        with_indices=True,\n",
    "        load_from_cache_file=False  \n",
    "    )\n",
    "\n",
    "updated = DatasetDict({split: rewrite_instr(ds) for split, ds in old_data.items()})\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(depths , bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Hypothesis Parse Tree Depth\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"qnli train - Hypothesis Depth Distribution\")\n",
    "plt.show()\n",
    "low = data[\"train\"].filter(lambda x: x[\"hypo_depth\"] <= 9)\n",
    "medium = data[\"train\"].filter(lambda x: 10 <= x[\"hypo_depth\"] <= 14)\n",
    "high = data[\"train\"].filter(lambda x: x[\"hypo_depth\"] >= 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset,DatasetDict\n",
    "test=pd.read_csv(\"test_with_parse_depth.csv\",encoding='utf-8-sig')\n",
    "test = Dataset.from_pandas(test)\n",
    "test_low = test.filter(lambda x: x[\"hypo_depth\"] <= 9)\n",
    "test_medium =test.filter(lambda x: 10 <= x[\"hypo_depth\"] <= 14)\n",
    "test_high = test.filter(lambda x: x[\"hypo_depth\"] >= 15)\n",
    "test_dist = test['hypo_depth']\n",
    "plt.hist(test_dist , bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Hypothesis Parse Tree Depth\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"qnli test- Hypothesis Depth Distribution\")\n",
    "plt.show()\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train_low\": low,\n",
    "    \"train_medium\": medium,\n",
    "    \"train_high\": high,\n",
    "    \"test\": test\n",
    "})\n",
    "dataset_dict.push_to_hub(\"haeunkim/curriculum_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7854cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=load_dataset(\"glue\",\"qnli\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f03343b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string='When talking about the German language, the term German dialects is only used for the traditional regional varieties.'\n",
    "string in d['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "low=updated['train_low']\n",
    "medium=updated['train_medium']\n",
    "high=updated['train_high']\n",
    "train_all = concatenate_datasets([low, medium, high])\n",
    "print(len(train_all))  # 총 데이터 개수 확인\n",
    "test=updated['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(low),len(medium),len(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = train_all.filter(lambda x: x['hypo_depth'] <= 8)\n",
    "medium = train_all.filter(lambda x: 9 <= x['hypo_depth'] <= 11)\n",
    "high = train_all.filter(lambda x: x['hypo_depth'] >= 12)\n",
    "len(low)+len(medium)+len(high),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train_low\": low,\n",
    "    \"train_medium\": medium,\n",
    "    \"train_high\": high,\n",
    "    \"test\": test\n",
    "})\n",
    "dataset_dict.push_to_hub(\"haeunkim/curriculum_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "\n",
    "def _fix_orphan_quotes(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s).strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    \n",
    "    for q in ['\"', \"'\"]:\n",
    "        starts = s.startswith(q)\n",
    "        ends   = s.endswith(q)\n",
    "        if starts and not ends:\n",
    "            return s[1:].lstrip()\n",
    "        if ends and not starts:\n",
    "            return s[:-1].rstrip()\n",
    "    return s\n",
    "\n",
    "def parse_chosen_rejected(raw) -> str:\n",
    "   \n",
    "    if isinstance(raw, list):\n",
    "        cleaned = [_fix_orphan_quotes(x) for x in raw]\n",
    "        return str(cleaned)\n",
    "\n",
    "    \n",
    "    s = \"\" if raw is None else str(raw).strip()\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "            if isinstance(obj, list):\n",
    "                cleaned = [_fix_orphan_quotes(x) for x in obj]\n",
    "                return str(cleaned)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    return _fix_orphan_quotes(s)\n",
    "\n",
    "def parse_output(raw: str) -> str:\n",
    "    \n",
    "    s = _fix_orphan_quotes(raw)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        if isinstance(s, list):\n",
    "            return \",\".join(str(x).strip() for x in s)\n",
    "        \n",
    "        return str(s).strip()\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "    \n",
    "#df = pd.read_csv(\"train_no_dup.csv\", encoding='utf-8')\n",
    "def create_curriculum_dpo(df):\n",
    "    dpo_data = []\n",
    "    for input_text, group in df.groupby(\"input\"):\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            if len(input_text.strip()) <= 3:\n",
    "                continue \n",
    "            sentence=parse_chosen_rejected(row['input'])\n",
    "            instruction=row['instruction']\n",
    "            prompt = f\"INSTRUCTION: {instruction.strip()}\\nINPUT: {sentence.strip()}\"\n",
    "\n",
    "            chosen = parse_chosen_rejected(row[\"output\"])\n",
    "            \n",
    "            candidates = group[group[\"instruction\"] != row[\"instruction\"]]\n",
    "            negatives = candidates[candidates[\"output\"].apply(lambda x: parse_chosen_rejected(x) != chosen)]\n",
    "\n",
    "           \n",
    "            if negatives.empty:\n",
    "                negatives = candidates\n",
    "\n",
    "            if negatives.empty:\n",
    "                continue\n",
    "\n",
    "            rejected = parse_chosen_rejected(random.choice(negatives[\"output\"].tolist()))\n",
    "            \n",
    "            dpo_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"instruction\":instruction,\n",
    "                \"input\":sentence,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected\n",
    "            })\n",
    "    return dpo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5863fbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_low: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'parse', 'hypo_depth'],\n",
       "        num_rows: 99450\n",
       "    })\n",
       "    train_medium: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'parse', 'hypo_depth'],\n",
       "        num_rows: 99195\n",
       "    })\n",
       "    train_high: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'parse', 'hypo_depth'],\n",
       "        num_rows: 96110\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'parse', 'hypo_depth'],\n",
       "        num_rows: 30670\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset,DatasetDict,Dataset, Features, Value\n",
    "import pandas as pd\n",
    "dataset=load_dataset(\"haeunkim/curriculum_learning\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b89e07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_tokens(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return (text.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "                .replace(\"-LSB-\", \"[\").replace(\"-RSB-\", \"]\")\n",
    "                .replace(\"-LCB-\", \"{\").replace(\"-RCB-\", \"}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "d31e939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_low = dataset['train_low'].to_pandas()\n",
    "df_me = dataset['train_medium'].to_pandas()\n",
    "df_high = dataset['train_high'].to_pandas()\n",
    "\n",
    "df_test=dataset['test'].to_pandas()\n",
    "\n",
    "for df_ in (df_low, df_me, df_high, df_test):\n",
    "    for col in ['input', 'output']:\n",
    "        if col in df_:\n",
    "            df_[col] = df_[col].apply(replace_special_tokens)\n",
    "    \n",
    "\n",
    "inputs = df_test['input'].tolist()\n",
    "seen = set()\n",
    "unique_inputs = [x for x in inputs if not (x in seen or seen.add(x))]\n",
    "\n",
    "# eval/test 비율 설정 (예: 50% / 50%)\n",
    "eval_inputs, test_inputs = train_test_split(unique_inputs, test_size=0.7, random_state=42)    \n",
    "eval_df = df_test[df_test['input'].isin(eval_inputs)].copy()\n",
    "test_df = df_test[df_test['input'].isin(test_inputs)].copy()\n",
    "\n",
    "dataset_low=create_curriculum_dpo(df_low)\n",
    "dataset_me=create_curriculum_dpo(df_me)    \n",
    "dataset_high=create_curriculum_dpo(df_high)\n",
    "dataset_test=create_curriculum_dpo(test_df)   \n",
    "dataset_eval=create_curriculum_dpo(eval_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "0bc7c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval DPO samples: 9200\n",
      "Test DPO samples: 21465\n"
     ]
    }
   ],
   "source": [
    "eval_dpo_low = Dataset.from_pandas(pd.DataFrame(dataset_low))\n",
    "eval_dpo_medium = Dataset.from_pandas(pd.DataFrame(dataset_me))\n",
    "eval_dpo_high = Dataset.from_pandas(pd.DataFrame(dataset_high))\n",
    "eval=Dataset.from_pandas(pd.DataFrame(dataset_eval))\n",
    "test=Dataset.from_pandas(pd.DataFrame(dataset_test))\n",
    "# 새로운 데이터셋 딕셔너리 생성\n",
    "new_dataset = DatasetDict({\n",
    "    'train_low':eval_dpo_low,\n",
    "    'train_medium':eval_dpo_medium,\n",
    "    'train_high':eval_dpo_high,\n",
    "    'eval': eval ,\n",
    "    'test': test\n",
    "})\n",
    "\n",
    "\n",
    "print(f\"Eval DPO samples: {len(eval)}\")\n",
    "print(f\"Test DPO samples: {len(test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "747285e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "#negative sample\n",
    "\n",
    "def left_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[1:]) if len(toks) >= 2 else \"\"\n",
    "\n",
    "def right_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[:-1]) if len(toks) >= 2 else \"\"\n",
    "\n",
    "def both_trim(span: str):\n",
    "    toks = span.split()\n",
    "    return \" \".join(toks[1:-1]) if len(toks) >= 3 else \"\"\n",
    "def get_negative_sample(sentence,chosen,reject,mode_weights=None):\n",
    "    modes, weights = zip(*mode_weights.items())\n",
    "    choice = random.choices(modes, weights, k=1)[0]\n",
    "    \n",
    "    \n",
    "   \n",
    "    len_sentence=len(sentence)\n",
    "    if choice == \"sentence_repeat\":\n",
    "        \n",
    "        repeat =random.randint(0,5)\n",
    "        rejected=[sentence] * repeat\n",
    "        \n",
    "        return json.dumps(rejected, ensure_ascii=False)\n",
    "        \n",
    "    elif choice == \"random\":\n",
    "        \n",
    "        start =random.randint(0,len_sentence)\n",
    "        end =random.randint(0,len_sentence)\n",
    "        rejected={\"start\": start, \"end\": end}\n",
    "        return json.dumps(rejected, ensure_ascii=False)\n",
    "        \n",
    "    elif choice == \"random_span\":\n",
    "        repeat =random.randint(0,5)\n",
    "        rejected=[]\n",
    "        for i in range(repeat):\n",
    "            start =random.randint(0,len_sentence)\n",
    "            end =random.randint(start,len_sentence)  \n",
    "            rejected.append(sentence[start:end])  \n",
    "        out = json.dumps(rejected, ensure_ascii=False) if rejected else \"[]\"\n",
    "        return out\n",
    "    elif choice == \"broken\":\n",
    "        rejected=[]\n",
    "        kind = random.choice([\"broken_left\", \"broken_right\"])\n",
    "        start =random.randint(0,len_sentence+1)\n",
    "        end =random.randint(start,len_sentence+1)\n",
    "        frag = sentence[start:end]  \n",
    "        if kind =='broken_right':\n",
    "           rejected.append(f'[\"{frag}')\n",
    "        else:\n",
    "            rejected.append(f'{frag}\"]')\n",
    "        return json.dumps(rejected, ensure_ascii=False) if rejected else \"[]\"\n",
    "    elif choice==\"trim\":\n",
    "        rejected=[]\n",
    "        for gold in chosen:\n",
    "            rn=random.randint(0,2)\n",
    "            if rn==0:\n",
    "                corrupted = left_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "            elif rn==1:\n",
    "                corrupted =right_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "            else:\n",
    "                corrupted = both_trim(gold)\n",
    "                rejected.append(corrupted)\n",
    "            return json.dumps(rejected, ensure_ascii=False) if rejected else \"[]\"\n",
    "    elif choice=='original':\n",
    "        \n",
    "        return json.dumps(reject, ensure_ascii=False) if reject else \"[]\"\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "b362d43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fce29245f348338854b5103451df0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/99395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2151e6e6ca904a4eb6f889edf6ad92ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/99195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae733a3081e43a5a4330e80b26204fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/96110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20a9270f023404fa7e700104d97f3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/9200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a2d5010cab46d1b3a4a50b5e0a882f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating negative samples:   0%|          | 0/21465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_negative_row(example):\n",
    "    \n",
    "    rejected = get_negative_sample(\n",
    "            sentence=example[\"input\"],\n",
    "            chosen=example[\"chosen\"],\n",
    "            reject=example['rejected'],\n",
    "            mode_weights ={\"sentence_repeat\":0.05,\"trim\": 0.05, \"random\": 0.05,\"random_span\":0.05,\"broken\": 0.05,\"original\":0.75}\n",
    "            \n",
    "        )#mode_weights = {\"sentence_repeat\":0.15,\"trim\": 0.15, \"random\": 0.20,\"random_span\":0.15, \"broken\": 0.15,\"original\":0.20}\n",
    "    return {'rejected':(rejected)}\n",
    "    \n",
    "\n",
    "processed_low = eval_dpo_low.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "\n",
    "processed_medium = eval_dpo_medium.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "\n",
    "processed_high = eval_dpo_high.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "processed_eval = eval.map(add_negative_row, desc=\"Generating negative samples\")\n",
    "processed_test = test.map(add_negative_row, desc=\"Generating negative samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "e7c9a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95357824faa4557b895e91c9beb4325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6892f477d6495285735e966e5ef99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6bfa3989a94c29ade4b739f5d3de23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50df283d6cc742dc89388c45d9dd45be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b057ab49d3d542928d3d6bb6a1f18172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d58a2018b654d478443d85bb82b4dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/97 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6551de392f648b9a7a4e0a8128c44c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed84181728f743ffa75a0f26d74e56fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58194f6a25541c890d341e34edb85ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35270677f951489c878f4c4747222223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a52e2f87cd4b63a8d5500f657a132f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/haeunkim/final_dataset/commit/780a9feba569858109565d271388e31efd3f04af', commit_message='Upload dataset', commit_description='', oid='780a9feba569858109565d271388e31efd3f04af', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train_low\": processed_low,\n",
    "    \"train_medium\": processed_medium,\n",
    "    \"train_high\": processed_high,\n",
    "    \"eval\":processed_eval,\n",
    "    \"test\":processed_test\n",
    "})\n",
    "new_dataset.push_to_hub(\"haeunkim/final_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae187d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07057a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#haeunkim/curriculum_learning -> normal sft\n",
    "# haeunkim/curriculum_learning_sft_dpo_negative (curriculum+dpo)\n",
    "# {\"sentence_repeat\":0.15,\"trim\": 0.15, \"random\": 0.20,\"random_span\":0.15, \"broken\": 0.15,\"original\":0.20} -> \n",
    "#haeunkim/curriculum_learning_dpo_negative(dpo only)\n",
    "#{\"sentence_repeat\":0.05,\"trim\": 0.05, \"random\": 0.05,\"random_span\":0.05,\"broken\": 0.05,\"original\":0.75} ->\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
